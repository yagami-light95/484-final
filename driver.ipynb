{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_model import *\n",
    "from Generate_Captcha import *\n",
    "import os\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run:\n",
    "\n",
    "    def __init__(self):\n",
    "        read = ReadData()\n",
    "        self.test_x, self.test_y, self.test_num = read.load_data(folder=Config.test_folder)\n",
    "        self.train_x, self.train_y, self.train_num = read.load_data(folder=Config.train_folder)\n",
    "        self.val_x, self.val_y, self.val_num = read.load_data(folder=Config.validation_folder)\n",
    "\n",
    "        print 'Images for train ：%d, for validation : %d, for test : %d' \\\n",
    "              % (self.train_num, self.val_num, self.test_num)\n",
    "\n",
    "        self.run_model()\n",
    "\n",
    "    @staticmethod\n",
    "    def next_batch(x, y, length):\n",
    "        if length % Config.batch_size == 0:\n",
    "            times = int(length / Config.batch_size)\n",
    "        else:\n",
    "            times = int(length / Config.batch_size) + 1\n",
    "\n",
    "        start_id = 0\n",
    "        for _ in range(times):\n",
    "            end_id = min(start_id + Config.batch_size, length)\n",
    "            batch_data = x[start_id:end_id]\n",
    "            batch_label = y[start_id:end_id]\n",
    "            start_id = end_id\n",
    "            yield batch_data, batch_label\n",
    "\n",
    "    @staticmethod\n",
    "    def feed_data(x, y, keep_prob, is_training=True):\n",
    "        feed_dict = {model.input_x: x,\n",
    "                     model.input_y: y,\n",
    "                     model.keep_prob: keep_prob,\n",
    "                     model.training: is_training}\n",
    "        return feed_dict\n",
    "\n",
    "    def evaluate(self, sess, val_x, val_y, val_size):\n",
    "        total_loss = 0.\n",
    "        total_acc = 0.\n",
    "\n",
    "        for x_, y_ in self.next_batch(val_x, val_y, val_size):\n",
    "            length = len(y_)\n",
    "            feed_dict = self.feed_data(x_, y_, 1.0, False)\n",
    "            val_acc, val_loss = sess.run([model.accuracy, model.loss], feed_dict=feed_dict)\n",
    "            total_acc += val_acc * length\n",
    "            total_loss += val_loss * length\n",
    "        return total_acc / val_size, total_loss / val_size\n",
    "\n",
    "    def run_model(self):\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=1)\n",
    "        if not os.path.exists(Config.saver_folder):\n",
    "            os.mkdir(Config.saver_folder)\n",
    "        save_path = os.path.join(Config.saver_folder, 'best_validation')\n",
    "\n",
    "        total_batch = 0\n",
    "        best_acc = 0\n",
    "        last_improved_step = 0\n",
    "        require_steps = 1000\n",
    "        flag = False\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(Config.Epoch):\n",
    "            print 'Epoch : %d' % (epoch + 1)\n",
    "            for x, y in self.next_batch(self.train_x, self.train_y, self.train_num):\n",
    "                feed_dict = self.feed_data(x, y, Config.keep_prob, True)\n",
    "                sess.run(model.train_step, feed_dict=feed_dict)\n",
    "\n",
    "                if total_batch % Config.print_per_batch == 0:\n",
    "                    # Output the accuracy and loss values on the validation and training sets\n",
    "                    feed_dict[model.keep_prob] = 1.0\n",
    "                    feed_dict[model.training] = False\n",
    "                    train_accuracy, train_loss = sess.run([model.accuracy, model.loss],\n",
    "                                                          feed_dict=feed_dict)\n",
    "                    val_acc, val_loss = self.evaluate(sess, self.val_x, self.val_y, self.val_num)\n",
    "\n",
    "                    if val_acc > best_acc:\n",
    "                        # record the best result\n",
    "                        best_acc = val_acc\n",
    "                        last_improved_step = total_batch\n",
    "                        # store the model\n",
    "                        saver.save(sess=sess, save_path=save_path)\n",
    "                        improved = '*'\n",
    "                    else:\n",
    "                        improved = ''\n",
    "\n",
    "                    msg = 'Step {:5}, train_acc:{:8.2%}, train_loss:{:6.2f},' \\\n",
    "                          ' val_acc:{:8.2%}, val_loss:{:6.2f}, improved:{:3}'\n",
    "                    print msg.format(total_batch, train_accuracy, train_loss, val_acc, val_loss, improved)\n",
    "\n",
    "                if total_batch % Config.save_per_batch == 0:\n",
    "                    # write in tensorboard\n",
    "                    feed_dict[model.keep_prob] = 1.0\n",
    "                    feed_dict[model.training] = False\n",
    "                    s = sess.run(model.merged_summary, feed_dict=feed_dict)\n",
    "                    model.writer.add_summary(s, total_batch)\n",
    "\n",
    "                if total_batch - last_improved_step > require_steps:\n",
    "                    flag = True\n",
    "                    break\n",
    "\n",
    "                total_batch += 1\n",
    "            if flag:\n",
    "                print 'No improvement for over %d steps, auto-stopping....' % require_steps\n",
    "                break\n",
    "        end_time = datetime.now()\n",
    "        time_diff = (end_time - start_time).seconds\n",
    "        print 'Time Usage : {:.2f} hours'.format(time_diff / 3600.0)\n",
    "        # print the accuracy on the test set\n",
    "        test_acc, test_loss = self.evaluate(sess, self.test_x, self.test_y, self.test_num)\n",
    "\n",
    "        print \"Test accuracy:{:8.2%}, loss:{:6.2f}\".format(test_acc, test_loss)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From cnn_model.py:46: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /Users/lightyagami/opt/anaconda3/envs/tff/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From cnn_model.py:71: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Images for train ：10000, for validation : 1000, for test : 500\n",
      "Epoch : 1\n",
      "Step     0, train_acc:  11.33%, train_loss:  0.88, val_acc:   9.25%, val_loss:  0.90, improved:*  \n",
      "Step    20, train_acc:  11.33%, train_loss:  0.50, val_acc:   9.58%, val_loss:  0.51, improved:*  \n",
      "Step    40, train_acc:  11.33%, train_loss:  0.42, val_acc:  10.22%, val_loss:  0.42, improved:*  \n",
      "Step    60, train_acc:  10.94%, train_loss:  0.36, val_acc:  10.00%, val_loss:  0.36, improved:   \n",
      "Step    80, train_acc:  10.55%, train_loss:  0.34, val_acc:   9.95%, val_loss:  0.34, improved:   \n",
      "Step   100, train_acc:  11.72%, train_loss:  0.33, val_acc:  10.15%, val_loss:  0.33, improved:   \n",
      "Step   120, train_acc:  10.55%, train_loss:  0.33, val_acc:  10.88%, val_loss:  0.33, improved:*  \n",
      "Step   140, train_acc:  12.50%, train_loss:  0.33, val_acc:  10.75%, val_loss:  0.33, improved:   \n",
      "Epoch : 2\n",
      "Step   160, train_acc:  12.50%, train_loss:  0.33, val_acc:  11.25%, val_loss:  0.33, improved:*  \n",
      "Step   180, train_acc:  10.16%, train_loss:  0.33, val_acc:  10.70%, val_loss:  0.33, improved:   \n",
      "Step   200, train_acc:   9.77%, train_loss:  0.33, val_acc:  11.53%, val_loss:  0.33, improved:*  \n",
      "Step   220, train_acc:  18.75%, train_loss:  0.33, val_acc:  10.97%, val_loss:  0.33, improved:   \n",
      "Step   240, train_acc:  12.89%, train_loss:  0.33, val_acc:  10.35%, val_loss:  0.33, improved:   \n",
      "Step   260, train_acc:  10.55%, train_loss:  0.33, val_acc:  11.23%, val_loss:  0.33, improved:   \n",
      "Step   280, train_acc:  12.11%, train_loss:  0.33, val_acc:  10.75%, val_loss:  0.33, improved:   \n",
      "Step   300, train_acc:  14.84%, train_loss:  0.32, val_acc:  11.58%, val_loss:  0.33, improved:*  \n",
      "Epoch : 3\n",
      "Step   320, train_acc:  12.89%, train_loss:  0.33, val_acc:  11.08%, val_loss:  0.33, improved:   \n",
      "Step   340, train_acc:  10.94%, train_loss:  0.32, val_acc:  11.12%, val_loss:  0.33, improved:   \n",
      "Step   360, train_acc:  12.50%, train_loss:  0.33, val_acc:  10.82%, val_loss:  0.33, improved:   \n",
      "Step   380, train_acc:  11.33%, train_loss:  0.33, val_acc:  11.75%, val_loss:  0.33, improved:*  \n",
      "Step   400, train_acc:  13.67%, train_loss:  0.32, val_acc:  11.38%, val_loss:  0.33, improved:   \n",
      "Step   420, train_acc:  14.06%, train_loss:  0.33, val_acc:  11.45%, val_loss:  0.33, improved:   \n",
      "Step   440, train_acc:   9.77%, train_loss:  0.33, val_acc:  12.62%, val_loss:  0.33, improved:*  \n",
      "Step   460, train_acc:  14.06%, train_loss:  0.33, val_acc:  12.45%, val_loss:  0.33, improved:   \n",
      "Epoch : 4\n",
      "Step   480, train_acc:  15.23%, train_loss:  0.32, val_acc:  11.92%, val_loss:  0.33, improved:   \n",
      "Step   500, train_acc:  14.06%, train_loss:  0.33, val_acc:  12.30%, val_loss:  0.33, improved:   \n",
      "Step   520, train_acc:  11.33%, train_loss:  0.33, val_acc:  12.20%, val_loss:  0.33, improved:   \n",
      "Step   540, train_acc:  13.28%, train_loss:  0.33, val_acc:  12.63%, val_loss:  0.33, improved:*  \n",
      "Step   560, train_acc:  11.72%, train_loss:  0.33, val_acc:  11.98%, val_loss:  0.33, improved:   \n",
      "Step   580, train_acc:  15.62%, train_loss:  0.33, val_acc:  12.77%, val_loss:  0.32, improved:*  \n",
      "Step   600, train_acc:  13.28%, train_loss:  0.32, val_acc:  13.60%, val_loss:  0.33, improved:*  \n",
      "Step   620, train_acc:  14.06%, train_loss:  0.33, val_acc:  11.65%, val_loss:  0.33, improved:   \n",
      "Epoch : 5\n",
      "Step   640, train_acc:  12.50%, train_loss:  0.32, val_acc:  12.98%, val_loss:  0.32, improved:   \n",
      "Step   660, train_acc:  16.02%, train_loss:  0.32, val_acc:  13.92%, val_loss:  0.33, improved:*  \n",
      "Step   680, train_acc:  14.84%, train_loss:  0.33, val_acc:  13.58%, val_loss:  0.32, improved:   \n",
      "Step   700, train_acc:  17.97%, train_loss:  0.32, val_acc:  14.27%, val_loss:  0.32, improved:*  \n",
      "Step   720, train_acc:  14.45%, train_loss:  0.32, val_acc:  14.32%, val_loss:  0.32, improved:*  \n",
      "Step   740, train_acc:  20.31%, train_loss:  0.32, val_acc:  15.20%, val_loss:  0.32, improved:*  \n",
      "Step   760, train_acc:  13.67%, train_loss:  0.32, val_acc:  14.70%, val_loss:  0.32, improved:   \n",
      "Step   780, train_acc:  13.67%, train_loss:  0.32, val_acc:  16.90%, val_loss:  0.32, improved:*  \n",
      "Epoch : 6\n",
      "Step   800, train_acc:  15.62%, train_loss:  0.33, val_acc:  17.25%, val_loss:  0.33, improved:*  \n",
      "Step   820, train_acc:  21.48%, train_loss:  0.32, val_acc:  17.22%, val_loss:  0.32, improved:   \n",
      "Step   840, train_acc:  17.58%, train_loss:  0.32, val_acc:  17.42%, val_loss:  0.32, improved:*  \n",
      "Step   860, train_acc:  22.27%, train_loss:  0.32, val_acc:  18.22%, val_loss:  0.32, improved:*  \n",
      "Step   880, train_acc:  17.58%, train_loss:  0.32, val_acc:  19.72%, val_loss:  0.32, improved:*  \n",
      "Step   900, train_acc:  17.58%, train_loss:  0.32, val_acc:  19.70%, val_loss:  0.32, improved:   \n",
      "Step   920, train_acc:  25.39%, train_loss:  0.31, val_acc:  20.95%, val_loss:  0.31, improved:*  \n",
      "Step   940, train_acc:  24.22%, train_loss:  0.31, val_acc:  22.02%, val_loss:  0.31, improved:*  \n",
      "Epoch : 7\n",
      "Step   960, train_acc:  27.73%, train_loss:  0.31, val_acc:  24.42%, val_loss:  0.31, improved:*  \n",
      "Step   980, train_acc:  26.56%, train_loss:  0.31, val_acc:  26.45%, val_loss:  0.31, improved:*  \n",
      "Step  1000, train_acc:  29.69%, train_loss:  0.30, val_acc:  27.00%, val_loss:  0.30, improved:*  \n",
      "Step  1020, train_acc:  33.59%, train_loss:  0.29, val_acc:  29.15%, val_loss:  0.30, improved:*  \n",
      "Step  1040, train_acc:  34.77%, train_loss:  0.29, val_acc:  31.58%, val_loss:  0.29, improved:*  \n",
      "Step  1060, train_acc:  35.94%, train_loss:  0.28, val_acc:  30.75%, val_loss:  0.29, improved:   \n",
      "Step  1080, train_acc:  35.16%, train_loss:  0.28, val_acc:  32.50%, val_loss:  0.29, improved:*  \n",
      "Epoch : 8\n",
      "Step  1100, train_acc:  39.06%, train_loss:  0.28, val_acc:  35.13%, val_loss:  0.28, improved:*  \n",
      "Step  1120, train_acc:  35.55%, train_loss:  0.28, val_acc:  38.80%, val_loss:  0.28, improved:*  \n",
      "Step  1140, train_acc:  40.23%, train_loss:  0.27, val_acc:  39.17%, val_loss:  0.28, improved:*  \n",
      "Step  1160, train_acc:  39.84%, train_loss:  0.27, val_acc:  39.27%, val_loss:  0.27, improved:*  \n",
      "Step  1180, train_acc:  44.14%, train_loss:  0.26, val_acc:  41.32%, val_loss:  0.27, improved:*  \n",
      "Step  1200, train_acc:  49.61%, train_loss:  0.26, val_acc:  44.23%, val_loss:  0.26, improved:*  \n",
      "Step  1220, train_acc:  50.39%, train_loss:  0.25, val_acc:  44.90%, val_loss:  0.26, improved:*  \n",
      "Step  1240, train_acc:  48.83%, train_loss:  0.25, val_acc:  46.25%, val_loss:  0.26, improved:*  \n",
      "Epoch : 9\n",
      "Step  1260, train_acc:  54.30%, train_loss:  0.24, val_acc:  49.68%, val_loss:  0.25, improved:*  \n",
      "Step  1280, train_acc:  52.73%, train_loss:  0.24, val_acc:  49.05%, val_loss:  0.25, improved:   \n",
      "Step  1300, train_acc:  57.81%, train_loss:  0.25, val_acc:  51.92%, val_loss:  0.25, improved:*  \n",
      "Step  1320, train_acc:  59.77%, train_loss:  0.24, val_acc:  53.23%, val_loss:  0.24, improved:*  \n",
      "Step  1340, train_acc:  51.56%, train_loss:  0.24, val_acc:  55.30%, val_loss:  0.24, improved:*  \n",
      "Step  1360, train_acc:  61.33%, train_loss:  0.22, val_acc:  56.55%, val_loss:  0.23, improved:*  \n",
      "Step  1380, train_acc:  57.81%, train_loss:  0.22, val_acc:  55.43%, val_loss:  0.23, improved:   \n",
      "Step  1400, train_acc:  62.11%, train_loss:  0.22, val_acc:  58.93%, val_loss:  0.22, improved:*  \n",
      "Epoch : 10\n",
      "Step  1420, train_acc:  65.23%, train_loss:  0.21, val_acc:  60.73%, val_loss:  0.22, improved:*  \n",
      "Step  1440, train_acc:  62.11%, train_loss:  0.21, val_acc:  62.65%, val_loss:  0.22, improved:*  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1460, train_acc:  64.45%, train_loss:  0.21, val_acc:  62.75%, val_loss:  0.22, improved:*  \n",
      "Step  1480, train_acc:  67.58%, train_loss:  0.20, val_acc:  64.42%, val_loss:  0.21, improved:*  \n",
      "Step  1500, train_acc:  65.23%, train_loss:  0.20, val_acc:  64.85%, val_loss:  0.21, improved:*  \n",
      "Step  1520, train_acc:  70.31%, train_loss:  0.19, val_acc:  66.20%, val_loss:  0.20, improved:*  \n",
      "Step  1540, train_acc:  66.02%, train_loss:  0.19, val_acc:  66.95%, val_loss:  0.20, improved:*  \n",
      "Step  1560, train_acc:  68.75%, train_loss:  0.20, val_acc:  67.70%, val_loss:  0.20, improved:*  \n",
      "Epoch : 11\n",
      "Step  1580, train_acc:  72.27%, train_loss:  0.18, val_acc:  68.70%, val_loss:  0.19, improved:*  \n",
      "Step  1600, train_acc:  69.92%, train_loss:  0.19, val_acc:  69.95%, val_loss:  0.19, improved:*  \n",
      "Step  1620, train_acc:  75.39%, train_loss:  0.17, val_acc:  70.83%, val_loss:  0.19, improved:*  \n",
      "Step  1640, train_acc:  76.95%, train_loss:  0.18, val_acc:  71.37%, val_loss:  0.19, improved:*  \n",
      "Step  1660, train_acc:  81.64%, train_loss:  0.17, val_acc:  72.32%, val_loss:  0.18, improved:*  \n",
      "Step  1680, train_acc:  72.66%, train_loss:  0.17, val_acc:  73.07%, val_loss:  0.18, improved:*  \n",
      "Step  1700, train_acc:  81.64%, train_loss:  0.16, val_acc:  74.00%, val_loss:  0.17, improved:*  \n",
      "Step  1720, train_acc:  80.47%, train_loss:  0.15, val_acc:  73.70%, val_loss:  0.17, improved:   \n",
      "Epoch : 12\n",
      "Step  1740, train_acc:  78.91%, train_loss:  0.16, val_acc:  75.05%, val_loss:  0.17, improved:*  \n",
      "Step  1760, train_acc:  76.17%, train_loss:  0.16, val_acc:  75.80%, val_loss:  0.17, improved:*  \n",
      "Step  1780, train_acc:  85.16%, train_loss:  0.15, val_acc:  75.30%, val_loss:  0.17, improved:   \n",
      "Step  1800, train_acc:  79.69%, train_loss:  0.16, val_acc:  77.45%, val_loss:  0.16, improved:*  \n",
      "Step  1820, train_acc:  79.30%, train_loss:  0.16, val_acc:  77.45%, val_loss:  0.16, improved:*  \n",
      "Step  1840, train_acc:  77.73%, train_loss:  0.15, val_acc:  77.80%, val_loss:  0.16, improved:*  \n",
      "Step  1860, train_acc:  85.94%, train_loss:  0.13, val_acc:  78.43%, val_loss:  0.15, improved:*  \n",
      "Step  1880, train_acc:  83.20%, train_loss:  0.14, val_acc:  78.67%, val_loss:  0.15, improved:*  \n",
      "Epoch : 13\n",
      "Step  1900, train_acc:  78.52%, train_loss:  0.14, val_acc:  78.57%, val_loss:  0.15, improved:   \n",
      "Step  1920, train_acc:  80.08%, train_loss:  0.14, val_acc:  78.78%, val_loss:  0.15, improved:*  \n",
      "Step  1940, train_acc:  83.59%, train_loss:  0.14, val_acc:  80.08%, val_loss:  0.15, improved:*  \n",
      "Step  1960, train_acc:  82.03%, train_loss:  0.14, val_acc:  80.00%, val_loss:  0.14, improved:   \n",
      "Step  1980, train_acc:  80.86%, train_loss:  0.14, val_acc:  79.72%, val_loss:  0.14, improved:   \n",
      "Step  2000, train_acc:  82.03%, train_loss:  0.13, val_acc:  79.28%, val_loss:  0.15, improved:   \n",
      "Step  2020, train_acc:  82.81%, train_loss:  0.13, val_acc:  79.67%, val_loss:  0.14, improved:   \n",
      "Step  2040, train_acc:  93.75%, train_loss:  0.09, val_acc:  80.88%, val_loss:  0.14, improved:*  \n",
      "Epoch : 14\n",
      "Step  2060, train_acc:  87.89%, train_loss:  0.12, val_acc:  80.40%, val_loss:  0.14, improved:   \n",
      "Step  2080, train_acc:  91.80%, train_loss:  0.11, val_acc:  80.85%, val_loss:  0.14, improved:   \n",
      "Step  2100, train_acc:  86.33%, train_loss:  0.11, val_acc:  81.05%, val_loss:  0.13, improved:*  \n",
      "Step  2120, train_acc:  84.77%, train_loss:  0.12, val_acc:  81.55%, val_loss:  0.13, improved:*  \n",
      "Step  2140, train_acc:  86.72%, train_loss:  0.11, val_acc:  81.42%, val_loss:  0.13, improved:   \n",
      "Step  2160, train_acc:  82.81%, train_loss:  0.13, val_acc:  82.10%, val_loss:  0.13, improved:*  \n",
      "Step  2180, train_acc:  88.28%, train_loss:  0.11, val_acc:  82.75%, val_loss:  0.13, improved:*  \n",
      "Epoch : 15\n",
      "Step  2200, train_acc:  88.28%, train_loss:  0.11, val_acc:  82.60%, val_loss:  0.13, improved:   \n",
      "Step  2220, train_acc:  85.16%, train_loss:  0.12, val_acc:  82.80%, val_loss:  0.12, improved:*  \n",
      "Step  2240, train_acc:  87.11%, train_loss:  0.11, val_acc:  83.20%, val_loss:  0.12, improved:*  \n",
      "Step  2260, train_acc:  89.06%, train_loss:  0.10, val_acc:  82.97%, val_loss:  0.12, improved:   \n",
      "Step  2280, train_acc:  89.45%, train_loss:  0.10, val_acc:  83.68%, val_loss:  0.12, improved:*  \n",
      "Step  2300, train_acc:  85.16%, train_loss:  0.11, val_acc:  82.55%, val_loss:  0.12, improved:   \n",
      "Step  2320, train_acc:  89.45%, train_loss:  0.10, val_acc:  83.08%, val_loss:  0.12, improved:   \n",
      "Step  2340, train_acc:  87.11%, train_loss:  0.10, val_acc:  84.02%, val_loss:  0.12, improved:*  \n",
      "Epoch : 16\n",
      "Step  2360, train_acc:  85.94%, train_loss:  0.10, val_acc:  83.50%, val_loss:  0.12, improved:   \n",
      "Step  2380, train_acc:  86.72%, train_loss:  0.10, val_acc:  83.58%, val_loss:  0.12, improved:   \n",
      "Step  2400, train_acc:  88.67%, train_loss:  0.09, val_acc:  84.27%, val_loss:  0.11, improved:*  \n",
      "Step  2420, train_acc:  91.02%, train_loss:  0.09, val_acc:  84.42%, val_loss:  0.11, improved:*  \n",
      "Step  2440, train_acc:  91.02%, train_loss:  0.08, val_acc:  84.75%, val_loss:  0.11, improved:*  \n",
      "Step  2460, train_acc:  89.06%, train_loss:  0.09, val_acc:  84.82%, val_loss:  0.11, improved:*  \n",
      "Step  2480, train_acc:  91.80%, train_loss:  0.08, val_acc:  85.05%, val_loss:  0.11, improved:*  \n",
      "Step  2500, train_acc:  88.28%, train_loss:  0.09, val_acc:  85.43%, val_loss:  0.11, improved:*  \n",
      "Epoch : 17\n",
      "Step  2520, train_acc:  89.84%, train_loss:  0.09, val_acc:  85.15%, val_loss:  0.11, improved:   \n",
      "Step  2540, train_acc:  89.45%, train_loss:  0.09, val_acc:  85.43%, val_loss:  0.10, improved:   \n",
      "Step  2560, train_acc:  88.28%, train_loss:  0.10, val_acc:  85.03%, val_loss:  0.11, improved:   \n",
      "Step  2580, train_acc:  90.62%, train_loss:  0.09, val_acc:  85.55%, val_loss:  0.11, improved:*  \n",
      "Step  2600, train_acc:  87.50%, train_loss:  0.09, val_acc:  85.15%, val_loss:  0.10, improved:   \n",
      "Step  2620, train_acc:  90.62%, train_loss:  0.09, val_acc:  85.58%, val_loss:  0.10, improved:*  \n",
      "Step  2640, train_acc:  92.97%, train_loss:  0.08, val_acc:  85.92%, val_loss:  0.10, improved:*  \n",
      "Step  2660, train_acc:  88.28%, train_loss:  0.09, val_acc:  86.00%, val_loss:  0.10, improved:*  \n",
      "Epoch : 18\n",
      "Step  2680, train_acc:  91.41%, train_loss:  0.08, val_acc:  86.22%, val_loss:  0.10, improved:*  \n",
      "Step  2700, train_acc:  92.97%, train_loss:  0.08, val_acc:  86.27%, val_loss:  0.10, improved:*  \n",
      "Step  2720, train_acc:  92.19%, train_loss:  0.08, val_acc:  86.40%, val_loss:  0.10, improved:*  \n",
      "Step  2740, train_acc:  91.80%, train_loss:  0.07, val_acc:  86.48%, val_loss:  0.10, improved:*  \n",
      "Step  2760, train_acc:  92.19%, train_loss:  0.07, val_acc:  85.43%, val_loss:  0.10, improved:   \n",
      "Step  2780, train_acc:  93.36%, train_loss:  0.07, val_acc:  86.20%, val_loss:  0.10, improved:   \n",
      "Step  2800, train_acc:  91.80%, train_loss:  0.07, val_acc:  86.33%, val_loss:  0.10, improved:   \n",
      "Step  2820, train_acc:  93.36%, train_loss:  0.07, val_acc:  86.65%, val_loss:  0.10, improved:*  \n",
      "Epoch : 19\n",
      "Step  2840, train_acc:  95.31%, train_loss:  0.06, val_acc:  86.85%, val_loss:  0.09, improved:*  \n",
      "Step  2860, train_acc:  94.53%, train_loss:  0.07, val_acc:  87.05%, val_loss:  0.09, improved:*  \n",
      "Step  2880, train_acc:  91.80%, train_loss:  0.07, val_acc:  86.58%, val_loss:  0.10, improved:   \n",
      "Step  2900, train_acc:  94.53%, train_loss:  0.06, val_acc:  87.10%, val_loss:  0.09, improved:*  \n",
      "Step  2920, train_acc:  94.14%, train_loss:  0.06, val_acc:  87.35%, val_loss:  0.09, improved:*  \n",
      "Step  2940, train_acc:  87.11%, train_loss:  0.09, val_acc:  87.25%, val_loss:  0.09, improved:   \n",
      "Step  2960, train_acc:  93.36%, train_loss:  0.07, val_acc:  86.95%, val_loss:  0.09, improved:   \n",
      "Step  2980, train_acc:  92.19%, train_loss:  0.06, val_acc:  87.70%, val_loss:  0.09, improved:*  \n",
      "Epoch : 20\n",
      "Step  3000, train_acc:  95.70%, train_loss:  0.06, val_acc:  87.00%, val_loss:  0.09, improved:   \n",
      "Step  3020, train_acc:  93.75%, train_loss:  0.06, val_acc:  87.82%, val_loss:  0.09, improved:*  \n",
      "Step  3040, train_acc:  94.92%, train_loss:  0.06, val_acc:  88.32%, val_loss:  0.09, improved:*  \n",
      "Step  3060, train_acc:  93.36%, train_loss:  0.07, val_acc:  87.70%, val_loss:  0.09, improved:   \n",
      "Step  3080, train_acc:  93.36%, train_loss:  0.06, val_acc:  87.90%, val_loss:  0.09, improved:   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  3100, train_acc:  98.05%, train_loss:  0.05, val_acc:  88.30%, val_loss:  0.08, improved:   \n",
      "Step  3120, train_acc:  93.75%, train_loss:  0.06, val_acc:  88.45%, val_loss:  0.08, improved:*  \n",
      "Epoch : 21\n",
      "Step  3140, train_acc:  91.80%, train_loss:  0.06, val_acc:  88.65%, val_loss:  0.08, improved:*  \n",
      "Step  3160, train_acc:  94.14%, train_loss:  0.05, val_acc:  88.35%, val_loss:  0.08, improved:   \n",
      "Step  3180, train_acc:  93.75%, train_loss:  0.07, val_acc:  88.22%, val_loss:  0.08, improved:   \n",
      "Step  3200, train_acc:  93.75%, train_loss:  0.06, val_acc:  88.33%, val_loss:  0.08, improved:   \n",
      "Step  3220, train_acc:  93.75%, train_loss:  0.06, val_acc:  88.30%, val_loss:  0.08, improved:   \n",
      "Step  3240, train_acc:  96.48%, train_loss:  0.04, val_acc:  88.63%, val_loss:  0.08, improved:   \n",
      "Step  3260, train_acc:  93.75%, train_loss:  0.06, val_acc:  88.80%, val_loss:  0.08, improved:*  \n",
      "Step  3280, train_acc:  92.97%, train_loss:  0.06, val_acc:  88.60%, val_loss:  0.08, improved:   \n",
      "Epoch : 22\n",
      "Step  3300, train_acc:  95.31%, train_loss:  0.05, val_acc:  89.22%, val_loss:  0.08, improved:*  \n",
      "Step  3320, train_acc:  95.70%, train_loss:  0.05, val_acc:  88.80%, val_loss:  0.08, improved:   \n",
      "Step  3340, train_acc:  94.14%, train_loss:  0.06, val_acc:  89.00%, val_loss:  0.08, improved:   \n",
      "Step  3360, train_acc:  95.31%, train_loss:  0.06, val_acc:  89.40%, val_loss:  0.08, improved:*  \n",
      "Step  3380, train_acc:  91.80%, train_loss:  0.06, val_acc:  89.37%, val_loss:  0.08, improved:   \n",
      "Step  3400, train_acc:  97.27%, train_loss:  0.05, val_acc:  89.28%, val_loss:  0.08, improved:   \n",
      "Step  3420, train_acc:  94.14%, train_loss:  0.05, val_acc:  89.58%, val_loss:  0.08, improved:*  \n",
      "Step  3440, train_acc:  95.31%, train_loss:  0.05, val_acc:  89.47%, val_loss:  0.08, improved:   \n",
      "Epoch : 23\n",
      "Step  3460, train_acc:  96.48%, train_loss:  0.05, val_acc:  89.42%, val_loss:  0.08, improved:   \n",
      "Step  3480, train_acc:  96.48%, train_loss:  0.04, val_acc:  89.30%, val_loss:  0.08, improved:   \n",
      "Step  3500, train_acc:  96.09%, train_loss:  0.04, val_acc:  89.80%, val_loss:  0.07, improved:*  \n",
      "Step  3520, train_acc:  96.88%, train_loss:  0.04, val_acc:  89.80%, val_loss:  0.08, improved:*  \n",
      "Step  3540, train_acc:  94.92%, train_loss:  0.05, val_acc:  89.75%, val_loss:  0.08, improved:   \n",
      "Step  3560, train_acc:  96.09%, train_loss:  0.04, val_acc:  89.65%, val_loss:  0.08, improved:   \n",
      "Step  3580, train_acc:  98.44%, train_loss:  0.04, val_acc:  89.70%, val_loss:  0.08, improved:   \n",
      "Step  3600, train_acc:  97.66%, train_loss:  0.03, val_acc:  90.57%, val_loss:  0.07, improved:*  \n",
      "Epoch : 24\n",
      "Step  3620, train_acc:  95.70%, train_loss:  0.04, val_acc:  89.85%, val_loss:  0.07, improved:   \n",
      "Step  3640, train_acc:  98.05%, train_loss:  0.04, val_acc:  90.05%, val_loss:  0.08, improved:   \n",
      "Step  3660, train_acc:  98.05%, train_loss:  0.04, val_acc:  90.42%, val_loss:  0.07, improved:   \n",
      "Step  3680, train_acc:  97.66%, train_loss:  0.04, val_acc:  90.23%, val_loss:  0.07, improved:   \n",
      "Step  3700, train_acc:  95.70%, train_loss:  0.04, val_acc:  90.25%, val_loss:  0.07, improved:   \n",
      "Step  3720, train_acc:  98.05%, train_loss:  0.04, val_acc:  90.03%, val_loss:  0.07, improved:   \n",
      "Step  3740, train_acc:  97.66%, train_loss:  0.04, val_acc:  90.33%, val_loss:  0.07, improved:   \n",
      "Step  3760, train_acc:  98.83%, train_loss:  0.03, val_acc:  90.15%, val_loss:  0.07, improved:   \n",
      "Epoch : 25\n",
      "Step  3780, train_acc:  96.88%, train_loss:  0.04, val_acc:  90.60%, val_loss:  0.07, improved:*  \n",
      "Step  3800, train_acc:  98.83%, train_loss:  0.03, val_acc:  90.58%, val_loss:  0.07, improved:   \n",
      "Step  3820, train_acc:  96.88%, train_loss:  0.04, val_acc:  90.35%, val_loss:  0.07, improved:   \n",
      "Step  3840, train_acc:  95.31%, train_loss:  0.04, val_acc:  90.92%, val_loss:  0.07, improved:*  \n",
      "Step  3860, train_acc:  98.05%, train_loss:  0.03, val_acc:  90.63%, val_loss:  0.07, improved:   \n",
      "Step  3880, train_acc:  98.05%, train_loss:  0.03, val_acc:  90.95%, val_loss:  0.07, improved:*  \n",
      "Step  3900, train_acc:  98.44%, train_loss:  0.03, val_acc:  90.85%, val_loss:  0.07, improved:   \n",
      "Step  3920, train_acc:  95.31%, train_loss:  0.04, val_acc:  91.07%, val_loss:  0.07, improved:*  \n",
      "Epoch : 26\n",
      "Step  3940, train_acc:  97.27%, train_loss:  0.04, val_acc:  90.35%, val_loss:  0.07, improved:   \n",
      "Step  3960, train_acc:  97.66%, train_loss:  0.03, val_acc:  91.15%, val_loss:  0.07, improved:*  \n",
      "Step  3980, train_acc:  98.44%, train_loss:  0.03, val_acc:  90.88%, val_loss:  0.07, improved:   \n",
      "Step  4000, train_acc:  97.27%, train_loss:  0.04, val_acc:  90.88%, val_loss:  0.07, improved:   \n",
      "Step  4020, train_acc:  99.22%, train_loss:  0.03, val_acc:  91.28%, val_loss:  0.07, improved:*  \n",
      "Step  4040, train_acc:  98.44%, train_loss:  0.03, val_acc:  91.30%, val_loss:  0.07, improved:*  \n",
      "Step  4060, train_acc:  98.05%, train_loss:  0.03, val_acc:  91.42%, val_loss:  0.07, improved:*  \n",
      "Step  4080, train_acc:  98.44%, train_loss:  0.03, val_acc:  91.15%, val_loss:  0.07, improved:   \n",
      "Epoch : 27\n",
      "Step  4100, train_acc:  98.05%, train_loss:  0.03, val_acc:  90.55%, val_loss:  0.07, improved:   \n",
      "Step  4120, train_acc:  95.70%, train_loss:  0.03, val_acc:  91.67%, val_loss:  0.07, improved:*  \n",
      "Step  4140, train_acc:  98.44%, train_loss:  0.03, val_acc:  91.33%, val_loss:  0.07, improved:   \n",
      "Step  4160, train_acc:  98.44%, train_loss:  0.03, val_acc:  91.45%, val_loss:  0.07, improved:   \n",
      "Step  4180, train_acc:  99.61%, train_loss:  0.02, val_acc:  91.77%, val_loss:  0.07, improved:*  \n",
      "Step  4200, train_acc:  97.66%, train_loss:  0.03, val_acc:  91.42%, val_loss:  0.07, improved:   \n",
      "Step  4220, train_acc:  98.05%, train_loss:  0.03, val_acc:  91.15%, val_loss:  0.07, improved:   \n",
      "Epoch : 28\n",
      "Step  4240, train_acc:  98.44%, train_loss:  0.03, val_acc:  91.60%, val_loss:  0.07, improved:   \n",
      "Step  4260, train_acc:  99.22%, train_loss:  0.02, val_acc:  91.07%, val_loss:  0.07, improved:   \n",
      "Step  4280, train_acc:  98.05%, train_loss:  0.03, val_acc:  91.73%, val_loss:  0.06, improved:   \n",
      "Step  4300, train_acc:  97.66%, train_loss:  0.03, val_acc:  91.48%, val_loss:  0.07, improved:   \n",
      "Step  4320, train_acc:  99.61%, train_loss:  0.02, val_acc:  91.48%, val_loss:  0.07, improved:   \n",
      "Step  4340, train_acc:  98.44%, train_loss:  0.02, val_acc:  91.72%, val_loss:  0.06, improved:   \n",
      "Step  4360, train_acc:  99.22%, train_loss:  0.02, val_acc:  91.50%, val_loss:  0.06, improved:   \n",
      "Step  4380, train_acc:  96.48%, train_loss:  0.03, val_acc:  92.03%, val_loss:  0.07, improved:*  \n",
      "Epoch : 29\n",
      "Step  4400, train_acc:  98.44%, train_loss:  0.02, val_acc:  91.92%, val_loss:  0.06, improved:   \n",
      "Step  4420, train_acc:  98.44%, train_loss:  0.02, val_acc:  91.77%, val_loss:  0.07, improved:   \n",
      "Step  4440, train_acc:  98.44%, train_loss:  0.03, val_acc:  91.80%, val_loss:  0.06, improved:   \n",
      "Step  4460, train_acc:  97.27%, train_loss:  0.02, val_acc:  91.95%, val_loss:  0.06, improved:   \n",
      "Step  4480, train_acc:  96.48%, train_loss:  0.03, val_acc:  91.90%, val_loss:  0.06, improved:   \n",
      "Step  4500, train_acc:  99.22%, train_loss:  0.02, val_acc:  91.70%, val_loss:  0.06, improved:   \n",
      "Step  4520, train_acc:  99.61%, train_loss:  0.02, val_acc:  91.63%, val_loss:  0.06, improved:   \n",
      "Step  4540, train_acc:  99.61%, train_loss:  0.02, val_acc:  92.00%, val_loss:  0.06, improved:   \n",
      "Epoch : 30\n",
      "Step  4560, train_acc:  98.44%, train_loss:  0.02, val_acc:  91.97%, val_loss:  0.06, improved:   \n",
      "Step  4580, train_acc:  99.22%, train_loss:  0.02, val_acc:  91.75%, val_loss:  0.06, improved:   \n",
      "Step  4600, train_acc:  97.66%, train_loss:  0.02, val_acc:  92.07%, val_loss:  0.06, improved:*  \n",
      "Step  4620, train_acc:  97.27%, train_loss:  0.02, val_acc:  91.97%, val_loss:  0.06, improved:   \n",
      "Step  4640, train_acc:  98.83%, train_loss:  0.02, val_acc:  92.23%, val_loss:  0.06, improved:*  \n",
      "Step  4660, train_acc:  98.44%, train_loss:  0.02, val_acc:  91.72%, val_loss:  0.06, improved:   \n",
      "Step  4680, train_acc:  98.83%, train_loss:  0.02, val_acc:  92.25%, val_loss:  0.06, improved:*  \n",
      "Step  4700, train_acc:  97.66%, train_loss:  0.03, val_acc:  92.12%, val_loss:  0.06, improved:   \n",
      "Epoch : 31\n",
      "Step  4720, train_acc:  97.27%, train_loss:  0.02, val_acc:  92.00%, val_loss:  0.06, improved:   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  4740, train_acc:  97.66%, train_loss:  0.02, val_acc:  92.07%, val_loss:  0.06, improved:   \n",
      "Step  4760, train_acc:  99.22%, train_loss:  0.02, val_acc:  92.20%, val_loss:  0.06, improved:   \n",
      "Step  4780, train_acc:  98.05%, train_loss:  0.02, val_acc:  91.53%, val_loss:  0.07, improved:   \n",
      "Step  4800, train_acc: 100.00%, train_loss:  0.02, val_acc:  92.22%, val_loss:  0.06, improved:   \n",
      "Step  4820, train_acc:  98.44%, train_loss:  0.02, val_acc:  92.20%, val_loss:  0.06, improved:   \n",
      "Step  4840, train_acc:  98.05%, train_loss:  0.02, val_acc:  92.12%, val_loss:  0.06, improved:   \n",
      "Step  4860, train_acc:  99.22%, train_loss:  0.02, val_acc:  92.30%, val_loss:  0.06, improved:*  \n",
      "Epoch : 32\n",
      "Step  4880, train_acc:  98.83%, train_loss:  0.02, val_acc:  92.10%, val_loss:  0.06, improved:   \n",
      "Step  4900, train_acc:  98.83%, train_loss:  0.02, val_acc:  92.40%, val_loss:  0.06, improved:*  \n",
      "Step  4920, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.03%, val_loss:  0.06, improved:   \n",
      "Step  4940, train_acc:  99.22%, train_loss:  0.02, val_acc:  92.35%, val_loss:  0.06, improved:   \n",
      "Step  4960, train_acc:  98.83%, train_loss:  0.03, val_acc:  91.75%, val_loss:  0.07, improved:   \n",
      "Step  4980, train_acc:  99.61%, train_loss:  0.02, val_acc:  92.05%, val_loss:  0.06, improved:   \n",
      "Step  5000, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.27%, val_loss:  0.06, improved:   \n",
      "Step  5020, train_acc:  99.22%, train_loss:  0.02, val_acc:  92.40%, val_loss:  0.06, improved:*  \n",
      "Epoch : 33\n",
      "Step  5040, train_acc:  99.22%, train_loss:  0.02, val_acc:  92.03%, val_loss:  0.06, improved:   \n",
      "Step  5060, train_acc:  98.44%, train_loss:  0.02, val_acc:  92.47%, val_loss:  0.06, improved:*  \n",
      "Step  5080, train_acc:  98.83%, train_loss:  0.02, val_acc:  92.40%, val_loss:  0.06, improved:   \n",
      "Step  5100, train_acc:  99.22%, train_loss:  0.02, val_acc:  92.70%, val_loss:  0.06, improved:*  \n",
      "Step  5120, train_acc:  98.83%, train_loss:  0.02, val_acc:  92.18%, val_loss:  0.06, improved:   \n",
      "Step  5140, train_acc:  99.22%, train_loss:  0.02, val_acc:  92.90%, val_loss:  0.06, improved:*  \n",
      "Step  5160, train_acc:  98.05%, train_loss:  0.02, val_acc:  92.73%, val_loss:  0.06, improved:   \n",
      "Step  5180, train_acc: 100.00%, train_loss:  0.00, val_acc:  92.60%, val_loss:  0.06, improved:   \n",
      "Epoch : 34\n",
      "Step  5200, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.25%, val_loss:  0.07, improved:   \n",
      "Step  5220, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.67%, val_loss:  0.06, improved:   \n",
      "Step  5240, train_acc:  99.22%, train_loss:  0.01, val_acc:  92.65%, val_loss:  0.06, improved:   \n",
      "Step  5260, train_acc:  98.83%, train_loss:  0.01, val_acc:  92.65%, val_loss:  0.06, improved:   \n",
      "Step  5280, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.35%, val_loss:  0.06, improved:   \n",
      "Step  5300, train_acc:  99.22%, train_loss:  0.02, val_acc:  92.58%, val_loss:  0.06, improved:   \n",
      "Step  5320, train_acc:  98.05%, train_loss:  0.01, val_acc:  92.67%, val_loss:  0.06, improved:   \n",
      "Epoch : 35\n",
      "Step  5340, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.58%, val_loss:  0.06, improved:   \n",
      "Step  5360, train_acc:  99.22%, train_loss:  0.01, val_acc:  91.90%, val_loss:  0.07, improved:   \n",
      "Step  5380, train_acc:  98.44%, train_loss:  0.02, val_acc:  92.45%, val_loss:  0.06, improved:   \n",
      "Step  5400, train_acc:  99.22%, train_loss:  0.01, val_acc:  92.25%, val_loss:  0.06, improved:   \n",
      "Step  5420, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.12%, val_loss:  0.06, improved:*  \n",
      "Step  5440, train_acc:  98.83%, train_loss:  0.02, val_acc:  92.67%, val_loss:  0.06, improved:   \n",
      "Step  5460, train_acc:  99.22%, train_loss:  0.01, val_acc:  92.17%, val_loss:  0.06, improved:   \n",
      "Step  5480, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.88%, val_loss:  0.06, improved:   \n",
      "Epoch : 36\n",
      "Step  5500, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.85%, val_loss:  0.06, improved:   \n",
      "Step  5520, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.18%, val_loss:  0.07, improved:   \n",
      "Step  5540, train_acc:  98.83%, train_loss:  0.01, val_acc:  92.85%, val_loss:  0.06, improved:   \n",
      "Step  5560, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.85%, val_loss:  0.06, improved:   \n",
      "Step  5580, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.30%, val_loss:  0.06, improved:*  \n",
      "Step  5600, train_acc:  99.22%, train_loss:  0.01, val_acc:  92.87%, val_loss:  0.06, improved:   \n",
      "Step  5620, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.25%, val_loss:  0.07, improved:   \n",
      "Step  5640, train_acc:  99.61%, train_loss:  0.02, val_acc:  93.22%, val_loss:  0.06, improved:   \n",
      "Epoch : 37\n",
      "Step  5660, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.03%, val_loss:  0.06, improved:   \n",
      "Step  5680, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.80%, val_loss:  0.07, improved:   \n",
      "Step  5700, train_acc:  98.44%, train_loss:  0.02, val_acc:  92.73%, val_loss:  0.06, improved:   \n",
      "Step  5720, train_acc:  98.44%, train_loss:  0.02, val_acc:  92.93%, val_loss:  0.06, improved:   \n",
      "Step  5740, train_acc:  98.83%, train_loss:  0.02, val_acc:  92.65%, val_loss:  0.06, improved:   \n",
      "Step  5760, train_acc:  98.83%, train_loss:  0.01, val_acc:  92.80%, val_loss:  0.06, improved:   \n",
      "Step  5780, train_acc:  98.83%, train_loss:  0.01, val_acc:  93.00%, val_loss:  0.06, improved:   \n",
      "Step  5800, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.03%, val_loss:  0.07, improved:   \n",
      "Epoch : 38\n",
      "Step  5820, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.30%, val_loss:  0.06, improved:   \n",
      "Step  5840, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.85%, val_loss:  0.06, improved:   \n",
      "Step  5860, train_acc:  99.22%, train_loss:  0.01, val_acc:  92.62%, val_loss:  0.06, improved:   \n",
      "Step  5880, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.83%, val_loss:  0.07, improved:   \n",
      "Step  5900, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.00%, val_loss:  0.06, improved:   \n",
      "Step  5920, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.55%, val_loss:  0.06, improved:   \n",
      "Step  5940, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.55%, val_loss:  0.06, improved:   \n",
      "Step  5960, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.23%, val_loss:  0.06, improved:   \n",
      "Epoch : 39\n",
      "Step  5980, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.45%, val_loss:  0.06, improved:   \n",
      "Step  6000, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.03%, val_loss:  0.06, improved:   \n",
      "Step  6020, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.98%, val_loss:  0.06, improved:   \n",
      "Step  6040, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.80%, val_loss:  0.07, improved:   \n",
      "Step  6060, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.68%, val_loss:  0.07, improved:   \n",
      "Step  6080, train_acc:  99.22%, train_loss:  0.01, val_acc:  92.85%, val_loss:  0.07, improved:   \n",
      "Step  6100, train_acc:  99.22%, train_loss:  0.01, val_acc:  93.10%, val_loss:  0.06, improved:   \n",
      "Step  6120, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.05%, val_loss:  0.06, improved:   \n",
      "Epoch : 40\n",
      "Step  6140, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.03%, val_loss:  0.06, improved:   \n",
      "Step  6160, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.95%, val_loss:  0.06, improved:   \n",
      "Step  6180, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.32%, val_loss:  0.06, improved:*  \n",
      "Step  6200, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.08%, val_loss:  0.06, improved:   \n",
      "Step  6220, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.90%, val_loss:  0.07, improved:   \n",
      "Step  6240, train_acc:  99.22%, train_loss:  0.01, val_acc:  93.20%, val_loss:  0.06, improved:   \n",
      "Step  6260, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.23%, val_loss:  0.06, improved:   \n",
      "Epoch : 41\n",
      "Step  6280, train_acc:  99.22%, train_loss:  0.01, val_acc:  93.40%, val_loss:  0.06, improved:*  \n",
      "Step  6300, train_acc:  99.22%, train_loss:  0.01, val_acc:  93.33%, val_loss:  0.06, improved:   \n",
      "Step  6320, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.55%, val_loss:  0.06, improved:*  \n",
      "Step  6340, train_acc:  99.22%, train_loss:  0.01, val_acc:  93.15%, val_loss:  0.07, improved:   \n",
      "Step  6360, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.50%, val_loss:  0.06, improved:   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  6380, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.98%, val_loss:  0.07, improved:   \n",
      "Step  6400, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.60%, val_loss:  0.06, improved:*  \n",
      "Step  6420, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.25%, val_loss:  0.06, improved:   \n",
      "Epoch : 42\n",
      "Step  6440, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.32%, val_loss:  0.06, improved:   \n",
      "Step  6460, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.83%, val_loss:  0.07, improved:   \n",
      "Step  6480, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.15%, val_loss:  0.07, improved:   \n",
      "Step  6500, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.58%, val_loss:  0.06, improved:   \n",
      "Step  6520, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.30%, val_loss:  0.06, improved:   \n",
      "Step  6540, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.15%, val_loss:  0.06, improved:   \n",
      "Step  6560, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.23%, val_loss:  0.06, improved:   \n",
      "Step  6580, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.35%, val_loss:  0.06, improved:   \n",
      "Epoch : 43\n",
      "Step  6600, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.35%, val_loss:  0.06, improved:   \n",
      "Step  6620, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.15%, val_loss:  0.07, improved:   \n",
      "Step  6640, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.45%, val_loss:  0.07, improved:   \n",
      "Step  6660, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.42%, val_loss:  0.07, improved:   \n",
      "Step  6680, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.40%, val_loss:  0.06, improved:   \n",
      "Step  6700, train_acc:  99.61%, train_loss:  0.01, val_acc:  92.90%, val_loss:  0.07, improved:   \n",
      "Step  6720, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.10%, val_loss:  0.06, improved:   \n",
      "Step  6740, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.45%, val_loss:  0.06, improved:   \n",
      "Epoch : 44\n",
      "Step  6760, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.35%, val_loss:  0.07, improved:   \n",
      "Step  6780, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.13%, val_loss:  0.07, improved:   \n",
      "Step  6800, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.10%, val_loss:  0.06, improved:   \n",
      "Step  6820, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.33%, val_loss:  0.07, improved:   \n",
      "Step  6840, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.50%, val_loss:  0.07, improved:   \n",
      "Step  6860, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.25%, val_loss:  0.06, improved:   \n",
      "Step  6880, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.20%, val_loss:  0.07, improved:   \n",
      "Step  6900, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.35%, val_loss:  0.07, improved:   \n",
      "Epoch : 45\n",
      "Step  6920, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.75%, val_loss:  0.06, improved:*  \n",
      "Step  6940, train_acc: 100.00%, train_loss:  0.01, val_acc:  92.97%, val_loss:  0.06, improved:   \n",
      "Step  6960, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.63%, val_loss:  0.06, improved:   \n",
      "Step  6980, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.45%, val_loss:  0.06, improved:   \n",
      "Step  7000, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.62%, val_loss:  0.06, improved:   \n",
      "Step  7020, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.18%, val_loss:  0.07, improved:   \n",
      "Step  7040, train_acc: 100.00%, train_loss:  0.00, val_acc:  93.48%, val_loss:  0.07, improved:   \n",
      "Step  7060, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.38%, val_loss:  0.07, improved:   \n",
      "Epoch : 46\n",
      "Step  7080, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.48%, val_loss:  0.07, improved:   \n",
      "Step  7100, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.52%, val_loss:  0.07, improved:   \n",
      "Step  7120, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.33%, val_loss:  0.07, improved:   \n",
      "Step  7140, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.30%, val_loss:  0.07, improved:   \n",
      "Step  7160, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.00%, val_loss:  0.07, improved:   \n",
      "Step  7180, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.42%, val_loss:  0.07, improved:   \n",
      "Step  7200, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.17%, val_loss:  0.07, improved:   \n",
      "Step  7220, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.78%, val_loss:  0.07, improved:*  \n",
      "Epoch : 47\n",
      "Step  7240, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.47%, val_loss:  0.07, improved:   \n",
      "Step  7260, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.23%, val_loss:  0.07, improved:   \n",
      "Step  7280, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.42%, val_loss:  0.07, improved:   \n",
      "Step  7300, train_acc: 100.00%, train_loss:  0.00, val_acc:  93.37%, val_loss:  0.07, improved:   \n",
      "Step  7320, train_acc:  99.61%, train_loss:  0.01, val_acc:  93.23%, val_loss:  0.07, improved:   \n",
      "Step  7340, train_acc: 100.00%, train_loss:  0.01, val_acc:  93.33%, val_loss:  0.07, improved:   \n",
      "Step  7360, train_acc: 100.00%, train_loss:  0.00, val_acc:  93.02%, val_loss:  0.07, improved:   \n",
      "Epoch : 48\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = CNN()\n",
    "    Run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
